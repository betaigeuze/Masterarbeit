<p class="text-font">
Now <b>this</b> is a plot, isn't it? What you see in figure 4 is a <i>heatmap</i> of the <i>pairwise distances</i> between the trees of the forest. In the background, our Python code calculated how similar each tree is to each other tree in the forest. The darker, the more similar 2 trees are. Hover over the plot to see how similar the trees were exactly.
The similarity measure we're using here is called the <i>graph edit distance</i>. In short, it counts how many operations are needed to transform one tree into the other. So the lower the score, the more similar the 2 trees are to each other.
But why would we do that, you might ask? Well, what we have here, is a so-called <i>similarity</i> or <i>distance matrix</i>. We can use this similarity matrix and run it through a clustering algorithm. We can then identify groups of trees in the forest that are especially alike. Compared to what we saw in the <i>Iris</i> example, the similarities in the <i>Digits</i> dataset are almost non-existent. For our cluster algorithm this will be a challenge and it indicates to us, that there simply are no clusters to be found in this example. We will certainly try, but see for yourself in the next section. <br>
One possible reason for <b>why</b> this happens is the aforementioned large amount of features in the <i>Digits</i> dataset. At a certain amount of features,
the trees in the forest will be so different from each other that you would need a forest of 1000 or even more trees to achieve what we saw with the <i>Iris</i> example. However, this is merely a hypothesis at this point and would need further investigation to be confirmed.

<p class="text-font">
Below we'll dive deeper into clustering. The algorithm we we're using is called <i>DBSCAN</i>. It's a <i>density-based</i> clustering algorithm, so it searches our distance matrix for groups of trees that are all similar to each other - which is exactly what we want.
Keep in mind however, that not all of our trees can be attributed to a cluster. That's because the DBSCAN algorithm can also identify trees as <i>noise</i> or <i>outliers</i>. This does not mean, that these trees are "bad" in any way, it just means, that they are mostly unique in the forest and that there are not a lot of trees, that are similar to them. In this case, this effect is even more pronounced. As expected, the <i>DBSCAN</i> algorithm has its' troubles to find any clusters at all. The few that we can identify with the current settings only achieve a very low <i>Silhouette Score</i> which further implies that there are no meaningful clusters to be found in this example. <br>

<p class="text-font">
The plot we are looking at in figure 5 is called a <i>Silhouette Plot</i>, showing us all of the trees in the forest that were attributed to a cluster. The <b>Silhouette Score</b> - indicated by the color of the bars - is a metric to measure a cluster assignment. It ranges between -1 and 1, where 1 means that a data point was likely assigned to the correct cluster and -1 means that it was probably falsely classified. Seeing the results in the Silhouette plot manifests our prediction from the similarity matrix.
<br>