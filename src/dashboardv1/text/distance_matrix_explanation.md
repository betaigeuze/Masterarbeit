Now **this** is a plot, isn't it? What you see here is a *heatmap* of the *parwise distances* between the trees of the forest. In the background, our Python code calculated how similar each tree is to each other tree in the forest. The greener, the more similar 2 trees are. Hover over the plot to see how similar the trees were exactly.
The similarity measure we're using here is called the *graph edit distance*. In short, in counts how many operations are needed to transform one tree into the other. So the lower the score, the more similar the 2 trees are to each other.
But would we do that, you might ask? Well, what we have here, is a so-called *similarity* or *distance matrix*. We can use this similarity matrix and run it through a clustering algorithm. We can then identify groups of trees in the forest that are especially alike. Take your time to inspect the plot on your own.  


Below we'll dive deeper into clustering. The algorithm we we're using is called *DBSCAN*. It's a *density-based* clustering algorithm, so it searches our distance matrix for groups of trees that are all similar to each other - which is exactly what we want.
Keep in mind however, that not all of our trees can be attributed to a cluster. That's because the DBSCAN algorithm can also identify trees as *noise* or *outliers*. In fact, in our example, only about 30 percent of the trees can be attributed to a cluster. The rest are considered noise by the DBSCAN algorithm. This does not mean, that these trees are "bad" in any way, it just means, that they are mostly unique in the forest and that there are not a lot of trees, that are similar to them.