## Tutorial on Random Forests:
Imagine you had a number of flowers, that you wanted to distinguish into 3 groups, we'll call them *classes*. For each flower, you have measurements of their petal and sepal lengths and widths. A powerful tool for such a task is a **Decision Tree**. The idea of a Decision Tree is to look at the different measurements, let's call them *features*, and decide which of them holds the most valuable information. Now, how would you determine that, should not be too important at this point. Let's assume you knew, that the petal width is the most important discerning factor for the decision of which class the flower can be associated with. What you can do now is find a value at which you could seperate all of your flowers in two groups such that you have as many flowers of one type in one of the groups and as many of the other type in the other group. Say, you were to seperate all of the flowers with a petal width of over 3 cm, into one group and all of the others, into another. You can now proceed on to the next important feature and do the exact same thing for both of the two new groups you just created. Then you go on to the resulting 4 groups and do the same with them...
You get the idea. This part of the process is called *training* the decision tree. The resulting figure is a decision tree. You could use this tree to make predictions on a whole different set of flowers. The problem is, that it would likely classify these flowers rather poorly. That's what's called overfitting! Now one of the ways to avoid this to happen would be to use a **Random Forest**. A Random Forest is a collection of decision trees, where each tree was trained on a slightly different data set and built with a marginally different process.